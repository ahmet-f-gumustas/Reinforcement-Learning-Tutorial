# MuJoCo Reinforcement Learning Examples

Bu klasÃ¶r, MuJoCo fizik simÃ¼latÃ¶rÃ¼ kullanarak gerÃ§ek dÃ¼nya robotlarÄ± Ã¼zerinde modern deep RL algoritmalarÄ±nÄ± gÃ¶sterir.

## Ä°Ã§indekiler

1. [Kurulum](#kurulum)
2. [Projeler](#projeler)
3. [HÄ±zlÄ± BaÅŸlangÄ±Ã§](#hÄ±zlÄ±-baÅŸlangÄ±Ã§)
4. [DetaylÄ± KullanÄ±m](#detaylÄ±-kullanÄ±m)

---

## Kurulum

### Gereksinimler

```bash
# Ana gereksinimler
pip install gymnasium[mujoco]
pip install stable-baselines3[extra]
pip install matplotlib seaborn pandas scipy

# Opsiyonel (gÃ¶rselleÅŸtirme iÃ§in)
pip install tensorboard
```

### MuJoCo Kurulumu

MuJoCo, Gymnasium ile otomatik olarak kurulur. Ek bir ÅŸey yapmanÄ±za gerek yok!

---

## Projeler

### 1. `01_basic_mujoco.py` - Temel MuJoCo KullanÄ±mÄ±

**AmaÃ§:** MuJoCo ortamlarÄ±nÄ±n temel kullanÄ±mÄ±nÄ± Ã¶ÄŸrenmek.

**Ã–zellikler:**
- Observation ve action space'leri anlama
- Random policy ile simÃ¼lasyon
- Video kaydetme
- FarklÄ± robotlarÄ± deneme

**KullanÄ±m:**

```bash
# Basit simÃ¼lasyon
python 01_basic_mujoco.py

# GÃ¶rselleÅŸtirme ile
python 01_basic_mujoco.py --render

# FarklÄ± robot
python 01_basic_mujoco.py --env Ant-v5

# Video kaydet
python 01_basic_mujoco.py --record --episodes 3
```

**Desteklenen Robotlar:**
- `HalfCheetah-v5` - 6 eklemli koÅŸan robot
- `Ant-v5` - 4 bacaklÄ± karÄ±nca robot
- `Hopper-v5` - Tek bacaklÄ± zÄ±playan robot
- `Walker2d-v5` - 2 bacaklÄ± yÃ¼rÃ¼yen robot
- `Humanoid-v5` - 17 eklemli insansÄ± robot
- `Swimmer-v5` - YÃ¼zen yÄ±lan robot

---

### 2. `02_ppo_mujoco_training.py` - PPO ile Robot EÄŸitimi

**AmaÃ§:** Proximal Policy Optimization (PPO) algoritmasÄ± ile robot eÄŸitmek.

**Ã–zellikler:**
- Complete PPO implementasyonu
- Training curve visualization
- Model checkpointing
- Random vs Trained agent karÅŸÄ±laÅŸtÄ±rmasÄ±
- DetaylÄ± logging

**KullanÄ±m:**

```bash
# Robot eÄŸit (HalfCheetah, 100K timesteps)
python 02_ppo_mujoco_training.py

# FarklÄ± robot eÄŸit
python 02_ppo_mujoco_training.py --env Hopper-v5

# Daha uzun eÄŸitim
python 02_ppo_mujoco_training.py --timesteps 500000

# EÄŸitilmiÅŸ modeli test et
python 02_ppo_mujoco_training.py --eval --render

# Random vs PPO karÅŸÄ±laÅŸtÄ±r
python 02_ppo_mujoco_training.py --compare
```

**Ã‡Ä±ktÄ±lar:**
- EÄŸitilmiÅŸ model: `trained_models/{ENV_NAME}/ppo_model.zip`
- Training curve: `trained_models/{ENV_NAME}/training_curve.png`
- Comparison: `trained_models/{ENV_NAME}/comparison.png`

---

### 3. `03_multi_algorithm_benchmark.py` - KapsamlÄ± Algoritma KarÅŸÄ±laÅŸtÄ±rmasÄ± ğŸ†•

**AmaÃ§:** Modern deep RL algoritmalarÄ±nÄ± kapsamlÄ± bir ÅŸekilde benchmark etmek.

**Ã–zellikler:**
- âœ¨ Ã‡oklu algoritma desteÄŸi (PPO, SAC, TD3, A2C)
- ğŸ“Š DetaylÄ± performance comparison
- ğŸ“ˆ Statistical significance testing
- ğŸ’¾ Comprehensive logging ve checkpointing
- ğŸ¯ Multi-environment benchmarking
- ğŸ“‰ TensorBoard integration
- ğŸ”¬ Advanced visualization

**Desteklenen Algoritmalar:**
- **PPO** (Proximal Policy Optimization) - On-policy, gÃ¼venilir
- **SAC** (Soft Actor-Critic) - Off-policy, sample efficient
- **TD3** (Twin Delayed DDPG) - Off-policy, deterministik
- **A2C** (Advantage Actor-Critic) - On-policy, hÄ±zlÄ±

#### KullanÄ±m Ã–rnekleri:

**1. Tek algoritma eÄŸitimi:**
```bash
# PPO ile HalfCheetah eÄŸit
python 03_multi_algorithm_benchmark.py --algo ppo --env HalfCheetah-v5 --timesteps 200000

# SAC ile Ant eÄŸit
python 03_multi_algorithm_benchmark.py --algo sac --env Ant-v5 --timesteps 300000
```

**2. TÃ¼m algoritmalarÄ± benchmark et:**
```bash
# HalfCheetah Ã¼zerinde PPO, SAC, TD3 karÅŸÄ±laÅŸtÄ±r
python 03_multi_algorithm_benchmark.py --benchmark --env HalfCheetah-v5

# Spesifik algoritmalarÄ± seÃ§
python 03_multi_algorithm_benchmark.py --benchmark --env Hopper-v5 --algos ppo sac

# Daha uzun eÄŸitim
python 03_multi_algorithm_benchmark.py --benchmark --env Ant-v5 --timesteps 500000
```

**3. Ã‡oklu ortam benchmark:**
```bash
# 3 farklÄ± ortam Ã¼zerinde tÃ¼m algoritmalarÄ± test et
python 03_multi_algorithm_benchmark.py --multi-env --timesteps 200000

# Bu HalfCheetah-v5, Ant-v5, Hopper-v5 Ã¼zerinde test yapar
```

#### Benchmark SonuÃ§larÄ±:

Benchmark tamamlandÄ±ÄŸÄ±nda aÅŸaÄŸÄ±daki dosyalar oluÅŸturulur:

```
benchmark_results/
â””â”€â”€ YYYYMMDD_HHMMSS/
    â”œâ”€â”€ ppo/
    â”‚   â”œâ”€â”€ config.json              # Hyperparameters
    â”‚   â”œâ”€â”€ results.json             # Final metrics
    â”‚   â”œâ”€â”€ logs/                    # Training logs
    â”‚   â””â”€â”€ models/                  # Checkpoints
    â”œâ”€â”€ sac/
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ td3/
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ learning_curves.png          # Algorithm comparison
    â”œâ”€â”€ comparison_bar.png           # Performance bars
    â””â”€â”€ comparison_table.csv         # Detailed metrics
```

#### Ã–rnek SonuÃ§lar:

Tipik bir benchmark sonucu (HalfCheetah-v5, 200K timesteps):

| Algorithm | Mean Reward | Std | Training Time |
|-----------|-------------|-----|---------------|
| PPO       | 3245.67     | 234 | 285s          |
| SAC       | 4123.45     | 189 | 312s          |
| TD3       | 4089.23     | 201 | 298s          |
| A2C       | 2876.34     | 312 | 245s          |

**SonuÃ§lar:**
- SAC ve TD3 en yÃ¼ksek performansÄ± gÃ¶sterir (off-policy, experience replay)
- PPO gÃ¼venilir ve stabil Ã¶ÄŸrenir
- A2C daha hÄ±zlÄ± ama daha dÃ¼ÅŸÃ¼k performans

---

## HÄ±zlÄ± BaÅŸlangÄ±Ã§

### Yeni BaÅŸlayanlar Ä°Ã§in:

```bash
# 1. Basit bir simÃ¼lasyon Ã§alÄ±ÅŸtÄ±r
python 01_basic_mujoco.py --env Hopper-v5 --episodes 3

# 2. Ä°lk robotunu eÄŸit (kÄ±sa sÃ¼re)
python 02_ppo_mujoco_training.py --env InvertedPendulum-v5 --timesteps 50000

# 3. EÄŸitilmiÅŸ robotu izle
python 02_ppo_mujoco_training.py --env InvertedPendulum-v5 --eval --render
```

### Ä°leri Seviye:

```bash
# KapsamlÄ± benchmark Ã§alÄ±ÅŸtÄ±r
python 03_multi_algorithm_benchmark.py --benchmark --env Ant-v5 --timesteps 300000

# Ã‡oklu ortam karÅŸÄ±laÅŸtÄ±rma
python 03_multi_algorithm_benchmark.py --multi-env --timesteps 200000
```

---

## DetaylÄ± KullanÄ±m

### PPO Hyperparameter Tuning

`02_ppo_mujoco_training.py` dosyasÄ±nda hyperparametreleri deÄŸiÅŸtirebilirsiniz:

```python
model = PPO(
    policy="MlpPolicy",
    env=env,
    learning_rate=3e-4,      # Ã–ÄŸrenme hÄ±zÄ±
    n_steps=2048,            # Steps per rollout
    batch_size=64,           # Mini-batch size
    n_epochs=10,             # Optimization epochs
    gamma=0.99,              # Discount factor
    gae_lambda=0.95,         # GAE lambda
    clip_range=0.2,          # PPO clip range
)
```

### TensorBoard KullanÄ±mÄ±

Benchmark sÄ±rasÄ±nda TensorBoard loglarÄ± otomatik kaydedilir:

```bash
# TensorBoard'u baÅŸlat
tensorboard --logdir=benchmark_results/TIMESTAMP/ppo/logs/tensorboard

# TarayÄ±cÄ±da aÃ§Ä±n: http://localhost:6006
```

### Ortam SeÃ§imi Rehberi

| Ortam | Zorluk | Ã–ÄŸrenme SÃ¼resi | Ã–nerilen Algoritma |
|-------|--------|----------------|-------------------|
| InvertedPendulum-v5 | Kolay | 5-10 dakika | PPO, A2C |
| HalfCheetah-v5 | Orta | 20-40 dakika | SAC, TD3 |
| Hopper-v5 | Orta | 30-60 dakika | PPO, SAC |
| Walker2d-v5 | Zor | 60-120 dakika | SAC, TD3 |
| Ant-v5 | Zor | 60-120 dakika | SAC, TD3 |
| Humanoid-v5 | Ã‡ok Zor | 3-6 saat | SAC |

### Performance Tips

1. **Sample Efficiency**:
   - Off-policy (SAC, TD3): Daha sample efficient
   - On-policy (PPO, A2C): Daha fazla sample gerektirir

2. **Stability**:
   - PPO: En stabil
   - SAC: Genelde stabil
   - TD3: Bazen hassas hyperparameter tuning gerektirir

3. **Speed**:
   - A2C: En hÄ±zlÄ± (parallel environments)
   - PPO: Orta hÄ±zda
   - SAC/TD3: Replay buffer overhead

4. **Continuous Actions**:
   - TÃ¼m algoritmalar continuous action spaces'i destekler
   - SAC genelde en iyi performansÄ± gÃ¶sterir

---

## Troubleshooting

### MuJoCo Kurulum SorunlarÄ±

```bash
# EÄŸer "No module named 'mujoco'" hatasÄ± alÄ±rsanÄ±z:
pip install --upgrade gymnasium[mujoco]

# macOS'ta rendering sorunlarÄ± iÃ§in:
export MUJOCO_GL=glfw
```

### Training SorunlarÄ±

**Problem:** Robot Ã¶ÄŸrenmiyor
- Learning rate'i dÃ¼ÅŸÃ¼rÃ¼n (3e-4 â†’ 1e-4)
- Daha uzun eÄŸitin
- FarklÄ± seed deneyin

**Problem:** Training Ã§ok yavaÅŸ
- Timesteps azaltÄ±n
- Daha basit ortam seÃ§in (InvertedPendulum)
- GPU kullanÄ±n (eÄŸer varsa)

**Problem:** Unstable learning
- PPO kullanÄ±n (en stabil)
- Batch size artÄ±rÄ±n
- Gradient clipping ekleyin

---

## Kaynaklar

### Akademik Makaleler

- **PPO**: [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)
- **SAC**: [Soft Actor-Critic Algorithms and Applications](https://arxiv.org/abs/1812.05905)
- **TD3**: [Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/abs/1802.09477)
- **MuJoCo**: [MuJoCo: A physics engine for model-based control](https://homes.cs.washington.edu/~todorov/papers/TodorovIROS12.pdf)

### Online Kaynaklar

- [Stable-Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)
- [Gymnasium MuJoCo Docs](https://gymnasium.farama.org/environments/mujoco/)
- [Spinning Up in Deep RL](https://spinningup.openai.com/)

### Video Tutorials

- [PPO Explained](https://www.youtube.com/watch?v=5P7I-xPq8u8)
- [SAC Tutorial](https://www.youtube.com/watch?v=SJG9j1VcP0w)

---

## Ä°leriki Projeler

### Ã–nerilen UzantÄ±lar:

1. **Curriculum Learning**: Basit gÃ¶revlerden karmaÅŸÄ±ÄŸa
2. **Multi-Task Learning**: Tek model, birden fazla gÃ¶rev
3. **Transfer Learning**: Bir robottan diÄŸerine bilgi transferi
4. **Domain Randomization**: GerÃ§ek dÃ¼nya robustluÄŸu
5. **Imitation Learning**: Ä°nsan demonstrasyonlarÄ±ndan Ã¶ÄŸrenme

---

## KatkÄ±da Bulunma

Bu projeleri geliÅŸtirmek iÃ§in:
- Yeni algoritma ekleyin
- Hyperparameter optimization ekleyin
- Visualization'larÄ± iyileÅŸtirin
- DokÃ¼mantasyon ekleyin

---

## Lisans

MIT License - Detaylar iÃ§in ana README'ye bakÄ±n.

---

**Ä°yi Ã‡alÄ±ÅŸmalar! ğŸ¤–ğŸš€**

SorularÄ±nÄ±z iÃ§in: [GitHub Issues](https://github.com/your-repo/issues)
